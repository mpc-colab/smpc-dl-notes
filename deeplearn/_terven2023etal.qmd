## A Comprehensive Review of YOLO (v1 to v8+)

> @terven2023etal present review and analysis of the evolution 
of the Yolo algorithm, with a focus on the innovations and 
contributions made by each iteration, as well as the major changes
in network architecture (and training tricks) which have been 
implemented over time. 

![A timeline of YOLO development](deeplearn/imgs/yolo_timeline.png){fig-align="center" width="80%"}

### Applications of YOLO

Yolo has proven invaluable for a number of different applications

* autonomous vehicles
  * enables quick identification and tracking of objects like vehicles, 
    pedestrians, bicycles and other obstacles
* action recognition 
* video surveillance
* sports analysis
* human-computer interaction
* crop, disease, pest detection and classsification
* face detection - biometrics, security, facial recognition
* cancer detection 
* skin segmentation 
* pill identification 
* remote sensing
  * satellite and aerial imagery object detection / classification
  * land use mapping
  * urban planning
* security systems
* smart transportation systems
* robotics and drones 

### Evaluation Metrics

Average Precision (AP) and Mean Average Precision (mAP) are the most
common metrics used in the object detection task. <span style="color:lightGreen"> 
It measures average precision across all categories, providing a single value to 
compare different models </span> 

#### How AP works

* mAP is the average precision for accuracy of predictions across all 
  classes of objects contained within an image
  * individual AP values are determined for each category separately.
* IOU (intersection over union)
  * measures the proportion of the predicted bounding box which overlaps
    which overlaps with the true bounding box

![Intersection over union in practice](deeplearn/imgs/iou_img.png){fig-align="center"}

Different methods are used to compute AP when evaluating object detection 
methods on the COCO and VOC datasets (PASCAL-VOC)

### Non-Maximum Suppression

A post-processing technique - reduces number of overlapping boxes and 
improves detection quality. Object detectors typically generate multiple 
bounding boxes around the same object. Non-max suppression picks the best
ones and gets rid of the others.

The algorithm for this is defined below:

![Non-Max Suppression Alg](deeplearn/imgs/nms_alg.png){fig-align="center" width=80%}

A useful visualization is also provided:

![Non-Max Supression Vis](deeplearn/imgs/nms_vis.png)

### YOLO

The original authors of YOLO titled it as such for the reason that it 
only required a single pass on the image to accomplish the detection 
task. This is contrast to the other approaches used by Fast R-CNN 
and sliding window methods. 

The output coordinates of the bounding box were detected using more
straightforward regression techniques

#### YOLOv1 {#sec-yolo1}

<span style="color:Crimson"> PASCAL-VOC AP: 63.4% </span>

YOLOv1 predicted all bounding boxes simultaneously by the following 
process:

1. divide image into $S \times S$ grid 
2. predict $B$ bounding boxes of the same class and confidence for $C$
   different classes per grid element
3. each bounding box had five values:
    1. $Pc$ - confidence score for the bounding box - how likely it 
        contains an object and the accuracy of the box
    2. $bx$ and $by$ - coordinates of center of box relative to grid
        cell. 
    3. $bh$ and $bw$ - height and width of box relative to full 
4. output an $S \times S \times (B \times 5 + C)$ tensor
5. (optional) NMS used to remove redundant bounding boxes

Here is an example of that output: 

![yolo output prediction](deeplearn/imgs/yolo_out.png){width=80% fig-align="center"}

##### v1 Architecture

**Normal Architecture**

* <span style="color:yellow"> 24 conv layers </span>
  * $1 \times 1$ conv layers are used - reduce number of feature maps and 
    keep parameters lower
  * <span style="color:pink"> leaky rectified linear unit activations
* <span style="color:orange"> 2 fc layers </span>
  * predict bounding box coordinates / probs
  * <span style="color:pink"> linear activation function for final layer </span>

**FastYOLO**

* Used 9 conv layers instead of 24 for greater speed (at the cost of reduced
  accuracy)

![yolo v1 architecture](deeplearn/imgs/yolov1_arc.png){width=80% fig-align="center"}

##### v1 Training

Basic training process:

1. pretrain first 20 layers at resolution $224 \times 224$ with *ImageNet* dataset
2. add last four layers with randomly initialized weights - fine tune model 
   with PASCAL VOC 2007 and PASCAL VOC 2012 at resolution $448 \times 448$

Loss functions: 

* scaling factors
  * $\lambda_{coord} = 5$ - gives more weight to boxes with objects
  * $\lambda_{noobj} = 0.5$ - reduces importance of boxes with no object
* localization loss:
  * first two terms 
  * computes error in predicted bounding box locations $(x,y)$ and $(w,h)$
  * only penalizes boxes with objects in them
* confidence loss:
  * confidence error when object is detected (third term)
  * confidence error when no object is in box (fourth term)
* classification loss:
  * squared error of class conditional probabilities for each class if an 
    object appears in the cell

![yolo v1 loss function](deeplearn/imgs/yolov1_loss.png){width=85% fig-align="center"}

#### YOLOv2 (YOLO 9000) {#sec-yolo2}

<span style="color:Crimson"> PASCAL-VOC AP: 78.6% </span>

**Improvements / Changes**

1. Batch normalization - included on all convolutional layers
2. Higher resolution classifier - pretrained model (224 x 224) 
   and then fine-tuned with ImageNet at a higher reoslution (448 x 448) 
   for ten epochs
3. fully convolutional - remove dense layers and use fully conv architecture
4. use anchor boxes to predict bounding boxes
   1. anchor box - box with predefined shapes for prototypical objects 
   2. defined for each grid cell
   3. system predicts coordinates and class for every anchor box

![yolo v2 anchor boxes](deeplearn/imgs/yolov2_anchor.png){width=80% fig-align="center"}

5. Dimension clusters - pick good anchor boxes using k-means clustering on the
   training bounding boxes - improves accuracy of bounding boxes
6. Direct Location Prediction 
7. Finer-grained features
   1. removed pooling layer - get feature 13 x 13 feature map for 416 x 416 images
   2. passthrough layer - 26 x 26 x 512 feature map -> stack adjacent features into 
      different channels
8. Multi-scale training - train on different input sizes to make model robust to 
   different input types

##### v2 Architecture

* backbone architecture -> <span style="color:Yellow"> Darknet-19 </span>
  * 19 conv layers
  * 5 max pool layers
    * non-linear operation - uses OT to perform efficiently
  * use $1 \times 1$ conv between $3 \times 3$ to reduce parameters
  * batch normalization to help convergence
  * object classification head (replaces last 4 conv layers of YOLOv1)
    * 1 conv layer (1000 filters)
    * GAP layer
    * Softmax classifier
  
![yolo v2 architecture](deeplearn/imgs/yolov2_arc.png){width=80% fig-align="center"}

#### YOLOv3 {#sec-yolov3}

<span style="color:Crimson"> MS COCO AP: 36.2% AP(50): 60.6% </span>

The code used to run YOLOv3 in `Torch` is provided at this 
[repository](https://github.com/ultralytics/yolov3/blob/master/models/yolo.py)

##### YOLOv3 Architecture

YOLOv3 makes use of a larger network architecture (backbone) called
Darknet-53. 

* replaces all max-pooling layers with **strided convolutions** and 
  added residual connections (what are residual connections?) - see 
  @sec-residual_connections for more information on this primitive.

![yolo v3 architecture](deeplearn/imgs/yolov3_arc.png){width=80% fig-align="center"}

The darknet architecture is presented here as well (visually):

![darknet 53 backbone](deeplearn/imgs/darknet_53_arc.png){width=80% fig-align="center"}

##### Multi-Scale predictions

* enables multi-scale predictions (predictions at multiple grid sizes)
* this helps to obtain finer detailed boxes (improves prediction of 
  smaller boxes)
* YOLOv3 generates three separate outputs: 
  * **y1**: $13 \times 13$ grid defines the output
  * **y2**: concatenating output after $(Res \times 4)$ with output 
    of $(Res \times 8)$ - upsampling occurs from **y1** since 
    the feature maps are of different sizes $(13 \times 13)$ and 
    $(26 \times 26)$
  * **y3**: upsample **y2** output to match $52\times 52$ feature 
    maps

##### Backbone, Neck, and Head 

After release of YOLOv3, object detectors began to be described in 
terms of the backbone, neck, and head 

![modern object detection architecture](deeplearn/imgs/high_level_yolo_v4.png){width=60% fig-align="center"}

**Backbone**

+ Extracts useful features from the input image. 
+ A convolutional nerual network trained on large-scale image 
  classifications task (ImageNet)
+ captures hierarchical features at different scales
  + low-level features - earlier layers
  + high-level features - deeper layers
  
**Neck**

+ aggregates / refines features extracted by backbone
  + enhance spatial / semantic information across different scales
  + includes conv layers
  + includes feature pyramid networks

**Head**

+ makes predictions based on features provided by backbone and neck
+ consists of task-specific subnetworks to perform *classification*,
  *localization*, *localization*, *instance segmentation* and 
  *pose estimation*
+ non-maximum supression filters out overlapping predictions 
  (retains only most confident detections)

#### YOLOv4 {#sec-yolov4}

<span style="color:Crimson"> MS COCO AP: 43.5% AP(50): 65.7% </span> 

The philosophy of YOLOv4 approaches optimization of the model into two 
categories: *bag of freebies* and *bag of specials*

**Bag of Freebies**:

+ increase training time / cost
+ do not affect inference time
+ examples include *data augmentation*

**Bag of Specials**:

+ increase inference time / cost
+ improve accuracy of the model (MaP)
+ examples include
  + <span style="color:LightBlue"> enlarging receptive field ??? </span>
  + <span style="color:LightBlue"> combining features ??? </span>
  + <span style="color:LightBlue"> post-processing ??? </span>

##### Model Improvements

+ **Enhanced network architecture with Bag-of-Specials**
  + backbone: **Darknet-53** + <span style="color:LightBlue"> Cross-stage Partial Connections
   (CSPNet) ?? </span> + <span style="color:LightBlue"> Mish Activation Function ?? </span>
    + *tested several backbone architectures to choose best option*
    + CSP reduces computation while maintaining accuracy
  + neck: <span style="color:LightBlue"> Spatial Pyramid Pooling (SPP)?? </span> + 
    **Multi-scale predictions** + modified <span style="color:LightBlue"> path aggregation
    network PANet </span> + <span style="color:LightBlue"> modified spatial attention module 
    (SAM) </span>
    + SPP increases receptive field without affecting inference speed
  + detection head: **Same anchors as YOLOv3**
+ **Advanced training with Bag-of-Freebies**
  + regular augmentation
    + random brightness, contrast, scaling, cropping, flipping, rotation
  + special
    + **mosaic augmentation**: 
      + combines four images into a single one
      + reduces need for large mini-batches for batch normalization
    + **DropBlock** regularization (instead of *Dropout*)
    + **CIoU** loss and Cross mini-batch nomralization **CmBN** for collecting 
      statistics from the entire batch instead of just mini-batches
      + these changes improve the **detector**
+ **Self-adversarial Training**
  + improves model robustness to perturbations
+ **Hyperparameter optimization with Genetic Algorithms**
  + <span style="color:LightBlue"> genetic algs </span> used on first 10% of periods
  + <span style="color:LightBlue"> cosine annealing scheduler </span> to alter learning rate 
    during training

##### YOLOv4 Architecture 

![yolo v4 architecture](deeplearn/imgs/yolov4_arc.png){width=70% fig-align="center"}