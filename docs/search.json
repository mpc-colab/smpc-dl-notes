[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Secure Multiparty Computation Meets Deep Learning",
    "section": "",
    "text": "Preface\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Secure Multiparty Computation Meets Deep Learning",
    "section": "Contents",
    "text": "Contents\nThe quarto book contains an organized structure of notes on a variety of secure multiparty computation protocols, implementation details, and a discussion of v2x applications and relevant deep learning models. The authors hope that any readers find these resources useful."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Secure Multiparty Computation Meets Deep Learning",
    "section": "Resources",
    "text": "Resources\nThe matcha editor is used to construct some of the mathematical diagrams shown in this book. In order to export a matcha diagram, you need to enter the full-screen mode for the diagram, and click on the “export” drop-down which becomes available. This will allow you to save the math diagram as a png image. We will make liberal use of these throughout the book, especially for explaining complex security primitives such as beaver’s triples and homomorphic encryption."
  },
  {
    "objectID": "secure_primitives.html",
    "href": "secure_primitives.html",
    "title": "Secure Multiparty Computation Primitives",
    "section": "",
    "text": "This section of the text discusses a number of secure multiparty computation protocols in-depth and provides sample implementation to get you started."
  },
  {
    "objectID": "secure_prim/beavers_triples.html#generating-beavers-triples",
    "href": "secure_prim/beavers_triples.html#generating-beavers-triples",
    "title": "1  Beaver’s Triples Explained",
    "section": "1.1 Generating Beaver’s Triples",
    "text": "1.1 Generating Beaver’s Triples\nThe first step in masking multiplications is to generate Beaver’s Triples. These are random values \\(a, b, c\\) such that \\(a \\cdot b = c\\)\n\n1.1.1 Step 1: Generating Inputs\nSuppose we have two parties, Alice and Bob. Here, we will suppose that Alice is the sender, and Bob is the receiver. All this really means is that Alice sends her values to Bob first, with Bob sending his values to Alice thereafter.\nWe begin by first generating a random pair \\((a_{i}, b_{i})\\) for each party where \\(i\\) corresponds to the party in question. We generate these from the field \\(F_{2}^{2}\\) which simply means that we select two random bits from \\(\\{0,1\\}\\)\n\\[\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\mathbb{F}_{2}^{2} \\overset{\\$}{\\rightarrow} (a_{A}, b_{A}) & \\overbrace{}^{\\&} & \\mathbb{F}_{2}^{2} \\overset{\\$}{\\rightarrow} (a_{B}, b_{B})\n\\end{array}\n\\]\nNext, Bob and Alice each generate random masking values \\(r_{A}\\) and \\(r_{B}\\) respectively.\n\\[\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\mathbb{F}_{2} \\overset{\\$}{\\rightarrow} r_{A} & \\space & \\mathbb{F}_{2} \\overset{\\$}{\\rightarrow} r_{B}\n\\end{array}\n\\]\n\n\n1.1.2 Step 2: Sending and Receiving Shares\nAfter these masking values are generated, Alice uses oblivious transfer (see Chapter 2) to send \\((r_{A}, r_{A}\\oplus a_{A})\\) to Bob.\nWhen Bob receives this, he obtains \\(b_{B}a_{A} \\oplus r_{A}\\) and then sends \\((r_{B}, r_{B} \\oplus a_{B})\\) to Alice\n\\[\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\text{Snd } = (r_{A}, r_{A}\\oplus a_{A}) & \\overset{OT}{\\longrightarrow} & \\text{Rcv } = b_{B}a_{A} \\oplus r_{A} \\\\\n    \\text{Rcv } = b_{A}a_{B} \\oplus r_{B} & \\overset{OT}{\\longleftarrow} & \\text{Snd } = (r_{B}, r_{B} \\oplus a_{B})\n\\end{array}\n\\]\n\n\n\n\nBeaver, D. 1991. “Efficient Multiparty Protocols Using Circuit Randomization.” Advances in Cryptology. https://link.springer.com/chapter/10.1007/3-540-46766-1_34."
  },
  {
    "objectID": "deep_learning.html",
    "href": "deep_learning.html",
    "title": "Deep Learning",
    "section": "",
    "text": "This portion of the text deals with deep learning and artificial intelligence applications. Specifically, it covers topics such as object detection and classification with computer vision, neural network primitives, and others. Other machine learning applications and import resources may also be included in this portion of the text"
  },
  {
    "objectID": "object_detection.html#ssd-single-shot-multibox-detector",
    "href": "object_detection.html#ssd-single-shot-multibox-detector",
    "title": "3  Object Detection Algorithms",
    "section": "3.1 SSD: Single Shot MultiBox Detector",
    "text": "3.1 SSD: Single Shot MultiBox Detector\n\nThe single shot multibox detector is an algorithm presented by W. Liu (2016) for the purpose of taking a 300 x 300 input and generating bounding boxes on objects of interest within the image. The paper is linked here"
  },
  {
    "objectID": "object_detection.html#a-comprehensive-review-of-yolo-v1-to-v8",
    "href": "object_detection.html#a-comprehensive-review-of-yolo-v1-to-v8",
    "title": "3  Object Detection Algorithms",
    "section": "3.2 A Comprehensive Review of YOLO (v1 to v8+)",
    "text": "3.2 A Comprehensive Review of YOLO (v1 to v8+)\n\nJ. Terven (2023) present review and analysis of the evolution of the Yolo algorithm, with a focus on the innovations and contributions made by each iteration, as well as the major changes in network architecture (and training tricks) which have been implemented over time.\n\n\n\n\nA timeline of YOLO development\n\n\n\n3.2.1 Applications of YOLO\nYolo has proven invaluable for a number of different applications\n\nautonomous vehicles\n\nenables quick identification and tracking of objects like vehicles, pedestrians, bicycles and other obstacles\n\naction recognition\nvideo surveillance\nsports analysis\nhuman-computer interaction\ncrop, disease, pest detection and classsification\nface detection - biometrics, security, facial recognition\ncancer detection\nskin segmentation\npill identification\nremote sensing\n\nsatellite and aerial imagery object detection / classification\nland use mapping\nurban planning\n\nsecurity systems\nsmart transportation systems\nrobotics and drones\n\n\n\n3.2.2 Evaluation Metrics\nAverage Precision (AP) and Mean Average Precision (mAP) are the most common metrics used in the object detection task.  It measures average precision across all categories, providing a single value to compare different models \n\n3.2.2.1 How AP works\n\nmAP is the average precision for accuracy of predictions across all classes of objects contained within an image\n\nindividual AP values are determined for each category separately.\n\nIOU (intersection over union)\n\nmeasures the proportion of the predicted bounding box which overlaps which overlaps with the true bounding box\n\n\n\n\n\nIntersection over union in practice\n\n\nDifferent methods are used to compute AP when evaluating object detection methods on the COCO and VOC datasets (PASCAL-VOC)\n\n\n\n3.2.3 Non-Maximum Suppression\nA post-processing technique - reduces number of overlapping boxes and improves detection quality. Object detectors typically generate multiple bounding boxes around the same object. Non-max suppression picks the best ones and gets rid of the others.\nThe algorithm for this is defined below:\n\n\n\nNon-Max Suppression Alg\n\n\nA useful visualization is also provided:\n\n\n\nNon-Max Supression Vis\n\n\n\n\n3.2.4 YOLO\nThe original authors of YOLO titled it as such for the reason that it only required a single pass on the image to accomplish the detection task. This is contrast to the other approaches used by Fast R-CNN and sliding window methods.\nThe output coordinates of the bounding box were detected using more straightforward regression techniques\n\n3.2.4.1 YOLOv1\n AP: 63.4% \nYOLOv1 predicted all bounding boxes simultaneously by the following process:\n\ndivide image into \\(S \\times S\\) grid\npredict \\(B\\) bounding boxes of the same class and confidence for \\(C\\) different classes per grid element\neach bounding box had five values:\n\n\\(Pc\\) - confidence score for the bounding box - how likely it contains an object and the accuracy of the box\n\\(bx\\) and \\(by\\) - coordinates of center of box relative to grid cell.\n\\(bh\\) and \\(bw\\) - height and width of box relative to full\n\noutput an \\(S \\times S \\times (B \\times 5 + C)\\) tensor\n(optional) NMS used to remove redundant bounding boxes\n\nHere is an example of that output:\n\n\n\nyolo output prediction\n\n\n\n3.2.4.1.1 v1 Architecture\nNormal Architecture\n\n 24 conv layers \n\n\\(1 \\times 1\\) conv layers are used - reduce number of feature maps and keep parameters lower\n leaky rectified linear unit activations\n\n 2 fc layers \n\npredict bounding box coordinates / probs\n linear activation function for final layer \n\n\nFastYOLO\n\nUsed 9 conv layers instead of 24 for greater speed (at the cost of reduced accuracy)\n\n\n\n\nyolo v1 architecture\n\n\n\n\n3.2.4.1.2 v1 Training\nBasic training process:\n\npretrain first 20 layers at resolution \\(224 \\times 224\\) with ImageNet dataset\nadd last four layers with randomly initialized weights - fine tune model with PASCAL VOC 2007 and PASCAL VOC 2012 at resolution \\(448 \\times 448\\)\n\nLoss functions:\n\nscaling factors\n\n\\(\\lambda_{coord} = 5\\) - gives more weight to boxes with objects\n\\(\\lambda_{noobj} = 0.5\\) - reduces importance of boxes with no object\n\nlocalization loss:\n\nfirst two terms\ncomputes error in predicted bounding box locations \\((x,y)\\) and \\((w,h)\\)\nonly penalizes boxes with objects in them\n\nconfidence loss:\n\nconfidence error when object is detected (third term)\nconfidence error when no object is in box (fourth term)\n\nclassification loss:\n\nsquared error of class conditional probabilities for each class if an object appears in the cell\n\n\n\n\n\nyolo v1 loss function\n\n\n\n\n\n3.2.4.2 YOLOv2 (YOLO 9000)\n AP: 78.6% \nImprovements / Changes\n\nBatch normalization - included on all convolutional layers\nHigher resolution classifier - pretrained model (224 x 224) and then fine-tuned with ImageNet at a higher reoslution (448 x 448) for ten epochs\nfully convolutional - remove dense layers and use fully conv architecture\nuse anchor boxes to predict bounding boxes\n\nanchor box - box with predefined shapes for prototypical objects\ndefined for each grid cell\nsystem predicts coordinates and class for every anchor box\n\n\n\n\n\nyolo v2 anchor boxes\n\n\n\nDimension clusters - pick good anchor boxes using k-means clustering on the training bounding boxes - improves accuracy of bounding boxes\nDirect Location Prediction\nFiner-grained features\n\nremoved pooling layer - get feature 13 x 13 feature map for 416 x 416 images\npassthrough layer - 26 x 26 x 512 feature map -&gt; stack adjacent features into different channels\n\nMulti-scale training - train on different input sizes to make model robust to different input types\n\n\n3.2.4.2.1 v2 Architecture\n\nbackbone architecture -&gt;  Darknet-19 \n\n19 conv layers\n5 max pool layers\n\nnon-linear operation - uses OT to perform efficiently\n\nuse \\(1 \\times 1\\) conv between \\(3 \\times 3\\) to reduce parameters\nbatch normalization to help convergence\nobject classification head (replaces last 4 conv layers of YOLOv1)\n\n1 conv layer (1000 filters)\nGAP layer\nSoftmax classifier\n\n\n\n\n\n\nyolo v2 architecture\n\n\n\n\n\n\nJ. Terven, D. Cordova-Esparaza. 2023. “A Comprehensive Review of YOLO: From YOLOv1 to YOLOv8 and Beyond.” ACM Computing Surveys. https://arxiv.org/pdf/2304.00501v1.pdf.\n\n\nW. Liu, D. Erhan, D. Anguelov. 2016. “SSD: Single Shot MultiBox Detector.” ECCV."
  },
  {
    "objectID": "v2x_apps.html",
    "href": "v2x_apps.html",
    "title": "V2X Applications",
    "section": "",
    "text": "This portion of the text deals specifically with discussions of V2X applications and papers which have tackled these sorts of the problems in the past."
  },
  {
    "objectID": "red_light.html#motivation",
    "href": "red_light.html#motivation",
    "title": "4  Red Light Violation Detection",
    "section": "4.1 Motivation",
    "text": "4.1 Motivation\nSecure Red Light Violation detection is an important application of secure machine learning protocols. Oftentimes, these systems will require the use of image segmentation and object recognition protocols. The images used for this task expose often-times sensitive data about individual users.\nIn the practical setting of interest, this means exposing license-plate information, associated vehicles, and location information available from the images themselves."
  },
  {
    "objectID": "red_light.html#v2i-algorithms-for-rlr-detection",
    "href": "red_light.html#v2i-algorithms-for-rlr-detection",
    "title": "4  Red Light Violation Detection",
    "section": "4.2 V2I Algorithms for RLR Detection",
    "text": "4.2 V2I Algorithms for RLR Detection\n\nThe authors of this paper have constructed V2I mechanisms for red light running (RLR) detection, wrong way entry (WWE), and an array of other import tasks in the context of V2X. See the citation Dokur and Katkoori (2022)\n\n\n4.2.1 Red Light Violation Detection Algorithm\nThe proposed system utilizes the following logic to detect whether a car will violate a red light.  A car which is approaching an intersection is connected to road-side units (RSUs)  which are installed at traffic lights in an intersection.\nEach light is said to be located at points \\(B(x_{2}, y_{2}, z_{2})\\), \\(C(x_{3}, y_{3}, z_{3})\\), \\(D(x_{4}, y_{4}, z_{4})\\) and \\(E(x_{5}, y_{5}, z_{5})\\) respectively.\nUnlike image-based systems, this system  assumes V2I communication between the traffic lights and the vehicle in question.  This means that the traffic state does not need to be determined by an image classifier. Rather, we already have this information by default."
  },
  {
    "objectID": "red_light.html#thao-et.al-on-traffic-violation-detection",
    "href": "red_light.html#thao-et.al-on-traffic-violation-detection",
    "title": "4  Red Light Violation Detection",
    "section": "4.3 Thao et.al on Traffic Violation Detection",
    "text": "4.3 Thao et.al on Traffic Violation Detection\n\nThe paper proposed by L. Thao (2022) introduces a mechanism for detecting red light violations automatically. There paper is titled: Automatic Traffic Red-Light Violation Detection Using AI\n\n\n4.3.1 Problem Setting\nThe reason AI technologies (image classification and detection) systems are better suited than standard sensor technologies is that they are able to operate more consistently, even when the number of vehicles in the setting increases dramatically.\n\n\n4.3.2 System Design and Solution Approach\nSeparate the task into three parts:\n\nvehicle violation detection\nred signal change monitoring\nvehicle recognition\n\nVehicle Violation Detection\nThe  YOLOv5s pretrained model (COCO dataset)  is used for detecting violating vehicles. After detecting violation, following frames are used to try and determine the license plate (identfy vehicle). See Section 3.2.4.1 for more information on the YOLO object detection model.\nBelow, a picture of the overall system flow is presented:\n\n\n\nsystem flow\n\n\n\nvehicle tracking - performed every 5 frames\n\nif IOU (intersection over union) of bounding box is close to one from a previous frame, then the car is assumed to be the same one from that frame.\n\nviolation line detection\n\nimage processing is used to determine traffic lines\nboundary lines are drawn onto frames captured by the camera later\n\ntraffic state detection\n\ncolor filters and image processing used to detect changes in the state of the traffic light\n\n\n\n\n4.3.3 Primary Contributions\n\nImplementation of modified YOLOv5s model\n\nused parameter changes from original model\nachieved following accuracy results:\n\n 82% - vehicle identification \n 90% - traffic signal status change \n 86% - violation detection \n\n\nBest Performing Architecture given below (v3 / v4)\n\n\n\n\nmodified Yolo architecture"
  },
  {
    "objectID": "red_light.html#goma-et.al-on-rlr-detection-with-ssd-single-shot-detector",
    "href": "red_light.html#goma-et.al-on-rlr-detection-with-ssd-single-shot-detector",
    "title": "4  Red Light Violation Detection",
    "section": "4.4 Goma et.al on RLR Detection with SSD (Single Shot Detector)",
    "text": "4.4 Goma et.al on RLR Detection with SSD (Single Shot Detector)\n\nWork by J. Goma (2020) demonstrates the ability to detect red-light-running and over-speeding with a high level of accuracy. Specifically, they achieve 100% accuracy on red light running detection and 92.1% accuracy for over-speeding violations. They accomplish these results using a CNN applied to an SSD (single deep neural network)\n\n\n4.4.1 Methods and System Design\n\n\n\n\nDokur, O., and S. Katkoori. 2022. “Vehicle-to-Infrastructure Based Algorithms for Traffic Light Detection, Red Light Violation, and Wrong-Way Entry Applications.” IEEE International Symposium on Smart Electronic Systems.\n\n\nJ. Goma, R. Bautista, M. Eviota. 2020. “Detecting Red-Light Runners (RLR) and Speeding Violtion Through Video Capture.” IEEE 7th International Conference on Industrial Engineering and Applications.\n\n\nL. Thao, N. Anh, D. Cuong. 2022. “Automatic Traffic Red-Light Violation Detection Using AI.” International Information and Engineering Technology Association."
  },
  {
    "objectID": "traffic_flow.html#attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecasting",
    "href": "traffic_flow.html#attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecasting",
    "title": "5  Traffic Flow Forecasting",
    "section": "5.1 Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting",
    "text": "5.1 Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting\n\nIn this paper, S. Guo (2019) proposed a method for traffic flow prediction which utilized an attention based spatial-temporal graph convolutional network. They aimed to model several time-dependencies: (1) recent, (2) daily-periodic, and (3) weekly-periodic dependencies. The attention mechanism captures spatial-temporal patterns in the traffic data and the spatial-temporal convolution is used to capture spatial patterns while standard convolutions describe temporal features.\n\n\n5.1.1 Core Contributions of ASTGCN\nDifficulties of the traffic forecasting problem\n\nit is difficult to handle unstable and nonlinear data\nprediction performance of models require extensive feature engineering\n\ndomain expertise is necessary\n\ncnn - spatial feature extraction from grid-based data, gcn - describe spatial correlation of grid based data\n\nfails to simultaneously model spatial temporal features and dynamic correlations of traffic data\n\n\nAddressing these issues:\n\ndevelop a spatial-temporal attention mechanism\n\nlearns dynamic spatial-temporal correlations of traffic data\ntemporal attention is applied to capture dynamic temporal correlations for different times\n\nDesign of spatial-temporal convolution module\n\nhas graph convolution for modeling graph structure\nhas convolution in temporal dimension (kind of like 3-d convolution)\n\n\n\n\n\n\nS. Guo, N. Feng, Y. Lin. 2019. “Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting.” The Thirty-Third AAAI Conference on Artificial Intelligence."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Beaver, D. 1991. “Efficient Multiparty Protocols Using Circuit\nRandomization.” Advances in Cryptology. https://link.springer.com/chapter/10.1007/3-540-46766-1_34.\n\n\nDokur, O., and S. Katkoori. 2022. “Vehicle-to-Infrastructure Based\nAlgorithms for Traffic Light Detection, Red Light Violation, and\nWrong-Way Entry Applications.” IEEE International Symposium\non Smart Electronic Systems.\n\n\nJ. Goma, R. Bautista, M. Eviota. 2020. “Detecting Red-Light\nRunners (RLR) and Speeding Violtion Through Video Capture.”\nIEEE 7th International Conference on Industrial Engineering and\nApplications.\n\n\nJ. Terven, D. Cordova-Esparaza. 2023. “A Comprehensive Review of\nYOLO: From YOLOv1 to YOLOv8 and Beyond.” ACM Computing\nSurveys. https://arxiv.org/pdf/2304.00501v1.pdf.\n\n\nL. Thao, N. Anh, D. Cuong. 2022. “Automatic Traffic Red-Light\nViolation Detection Using AI.” International Information and\nEngineering Technology Association.\n\n\nS. Guo, N. Feng, Y. Lin. 2019. “Attention Based Spatial-Temporal\nGraph Convolutional Networks for Traffic Flow Forecasting.”\nThe Thirty-Third AAAI Conference on Artificial Intelligence.\n\n\nW. Liu, D. Erhan, D. Anguelov. 2016. “SSD: Single Shot MultiBox\nDetector.” ECCV."
  }
]