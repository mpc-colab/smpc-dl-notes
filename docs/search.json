[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Secure Multiparty Computation Meets Deep Learning",
    "section": "",
    "text": "Preface\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Secure Multiparty Computation Meets Deep Learning",
    "section": "Contents",
    "text": "Contents\nThe quarto book contains an organized structure of notes on a variety of secure multiparty computation protocols, implementation details, and a discussion of v2x applications and relevant deep learning models. The authors hope that any readers find these resources useful.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Secure Multiparty Computation Meets Deep Learning",
    "section": "Resources",
    "text": "Resources\nThe matcha editor is used to construct some of the mathematical diagrams shown in this book. In order to export a matcha diagram, you need to enter the full-screen mode for the diagram, and click on the “export” drop-down which becomes available. This will allow you to save the math diagram as a png image. We will make liberal use of these throughout the book, especially for explaining complex security primitives such as beaver’s triples and homomorphic encryption.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "secure_primitives.html",
    "href": "secure_primitives.html",
    "title": "Secure Multiparty Computation Primitives",
    "section": "",
    "text": "This section of the text discusses a number of secure multiparty computation protocols in-depth and provides sample implementation to get you started.",
    "crumbs": [
      "Secure Multiparty Computation Primitives"
    ]
  },
  {
    "objectID": "secure_prim/beavers_triples.html#generating-beavers-triples",
    "href": "secure_prim/beavers_triples.html#generating-beavers-triples",
    "title": "1  Beaver’s Triples Explained",
    "section": "1.1 Generating Beaver’s Triples",
    "text": "1.1 Generating Beaver’s Triples\nThe first step in masking multiplications is to generate Beaver’s Triples. These are random values a, b, c such that a \\cdot b = c\n\n1.1.1 Step 1: Generating Inputs\nSuppose we have two parties, Alice and Bob. Here, we will suppose that Alice is the sender, and Bob is the receiver. All this really means is that Alice sends her values to Bob first, with Bob sending his values to Alice thereafter.\nWe begin by first generating a random pair (a_{i}, b_{i}) for each party where i corresponds to the party in question. We generate these from the field F_{2}^{2} which simply means that we select two random bits from \\{0,1\\}\n\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\mathbb{F}_{2}^{2} \\overset{\\$}{\\rightarrow} (a_{A}, b_{A}) & \\overbrace{}^{\\&} & \\mathbb{F}_{2}^{2} \\overset{\\$}{\\rightarrow} (a_{B}, b_{B})\n\\end{array}\n\nNext, Bob and Alice each generate random masking values r_{A} and r_{B} respectively.\n\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\mathbb{F}_{2} \\overset{\\$}{\\rightarrow} r_{A} & \\space & \\mathbb{F}_{2} \\overset{\\$}{\\rightarrow} r_{B}\n\\end{array}\n\n\n\n1.1.2 Step 2: Sending and Receiving Shares\nAfter these masking values are generated, Alice uses oblivious transfer (see Chapter 2) to send (r_{A}, r_{A}\\oplus a_{A}) to Bob.\nWhen Bob receives this, he obtains b_{B}a_{A} \\oplus r_{A} and then sends (r_{B}, r_{B} \\oplus a_{B}) to Alice\n\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\text{Snd } = (r_{A}, r_{A}\\oplus a_{A}) & \\overset{OT}{\\longrightarrow} & \\text{Rcv } = b_{B}a_{A} \\oplus r_{A} \\\\\n    \\text{Rcv } = b_{A}a_{B} \\oplus r_{B} & \\overset{OT}{\\longleftarrow} & \\text{Snd } = (r_{B}, r_{B} \\oplus a_{B})\n\\end{array}\n\nAfter receiving these shares, Alice and Bob each received shares from one another, they XOR the product (intersection) of their shares with the ones received from the other party\n\n\\begin{array}{ccc}\n    \\text{Alice} & & \\text{Bob} \\\\\n    c_{A}: \\overbrace{r_{A}\\oplus a_{A}b_{A}}{\\text{Alice's og share}}\\oplus b_{A}a_{B} \\oplus r_{B} & &\n    c_{B}: \\overbrace{r_{B}\\oplus a_{B}b_{B}}{\\text{Bob's og share}}\\oplus b_{B}a_{A} \\oplus r_{A} \\\\\n    \\downarrow & & \\downarrow \\\\\n    c_{A}:\\text{---}{r_{A}}\\text{---}\\oplus a_{A}b_{A} \\oplus b_{A}a_{B} \\oplus \\text{---}{r_{B}}\\text{---} & & \\\\\n    c_{B}: \\text{---}{r_{B}}\\text{---}\\oplus a_{B}b_{B} \\oplus b_{B}a_{A} \\oplus \\text{---}{r_{A}}\\text{---}\n\\end{array}\n\n\n\n1.1.3 Step 3: Final Simplification\nNow that each party has obtaiined a preliminary share of C, C_{A} and C_{B} possessed by Alice and Bob respectively, we take the XOR of the two components to get the product a \\cdot b = c\nFirst, let us simplify c_{A} and c_{B} a bit more\n\nc_{A} = a_{A}b_{A}\\oplus b_{A}a_{B} \\\\[5pt]\na_{A} \\oplus a_{B} = a \\space\\space\\space b_{A} \\oplus b_{A} = b_{A} \\\\[15pt]\nc_{B} = a_{B}b_{B}\\oplus b_{B}a_{A} \\\\[5pt]\na_{B} \\oplus a_{B} = a \\space\\space\\space b_{B} \\oplus b_{B} = b_{B} \\\\[15pt]\nc_{A} = a(b_{A}) \\space\\space\\space c_{B} = a(b_{B})\n\nNow that we have simplified these terms, we can compute the final result\n\nc = c_{A} \\oplus c_{B} \\\\\nc = a(b_{A}) \\oplus a(b_{B}) \\\\\nc = a(b_{A} \\oplus b_{B}) \\\\\nc = a\\cdot b\n\nThus, we have shown how beaver’s triples are randomly generated to achieve the desired output.\n\n\n\n\nBeaver, D. 1991. “Efficient Multiparty Protocols Using Circuit Randomization.” Advances in Cryptology. https://link.springer.com/chapter/10.1007/3-540-46766-1_34."
  },
  {
    "objectID": "deep_learning.html",
    "href": "deep_learning.html",
    "title": "Deep Learning",
    "section": "",
    "text": "This portion of the text deals with deep learning and artificial intelligence applications. Specifically, it covers topics such as object detection and classification with computer vision, neural network primitives, and others. Other machine learning applications and import resources may also be included in this portion of the text"
  },
  {
    "objectID": "deeplearn/deep_primitives/residual_connection.html#motivation-for-residual-connections",
    "href": "deeplearn/deep_primitives/residual_connection.html#motivation-for-residual-connections",
    "title": "3  Residual connections",
    "section": "3.1 Motivation for Residual connections",
    "text": "3.1 Motivation for Residual connections\nDeep Neural Networks such as YOLO (see Section 4.2.4.3) allow for greater accuracy and performance. However, deep networks like this make it more difficult for the model to converge during training.\nResidual connections help to make training networks easier."
  },
  {
    "objectID": "deeplearn/deep_primitives/residual_connection.html#formulation",
    "href": "deeplearn/deep_primitives/residual_connection.html#formulation",
    "title": "3  Residual connections",
    "section": "3.2 Formulation",
    "text": "3.2 Formulation\nResidual connections - allow data to reach other parts of a sequential network by  skipping layers \nThis flow is depicted well by the following image:\n\n\n\nresidual connection diagram\n\n\nSteps:\n\nApply identity mapping to x - perform element-wise addition F(x) + x: this is the residual block\n\nresidual blocks may also include a ReLU actiation applied to F(x) + x. *This works when dimensions of F(x) and x are the same\nIf dimensions of F(x) and x are NOT the same, then you can multiply x by some matrix of constants W to scale it. F(x) + Wx"
  },
  {
    "objectID": "deeplearn/deep_primitives/residual_connection.html#the-utility-of-residual-blocks-for-training-deep-networks",
    "href": "deeplearn/deep_primitives/residual_connection.html#the-utility-of-residual-blocks-for-training-deep-networks",
    "title": "3  Residual connections",
    "section": "3.3 The Utility of Residual Blocks for Training Deep Networks",
    "text": "3.3 The Utility of Residual Blocks for Training Deep Networks\nEmpirical results have demonstrated that residual blocks increase the speed and ease of network convergence. There are a number of suspected reasons as to why this enables such performance gains.\n\n3.3.1 Ensemble of Shallow Neural Networks\n\n\n3.3.2 \n\n\n\n\nWong, W. 2021. “What Is Residual Connection?” https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55."
  },
  {
    "objectID": "object_detection.html#ssd-single-shot-multibox-detector",
    "href": "object_detection.html#ssd-single-shot-multibox-detector",
    "title": "4  Object Detection Algorithms",
    "section": "4.1 SSD: Single Shot MultiBox Detector",
    "text": "4.1 SSD: Single Shot MultiBox Detector\n\nThe single shot multibox detector is an algorithm presented by W. Liu (2016) for the purpose of taking a 300 x 300 input and generating bounding boxes on objects of interest within the image. The paper is linked here"
  },
  {
    "objectID": "object_detection.html#a-comprehensive-review-of-yolo-v1-to-v8",
    "href": "object_detection.html#a-comprehensive-review-of-yolo-v1-to-v8",
    "title": "4  Object Detection Algorithms",
    "section": "4.2 A Comprehensive Review of YOLO (v1 to v8+)",
    "text": "4.2 A Comprehensive Review of YOLO (v1 to v8+)\n\nJ. Terven (2023) present review and analysis of the evolution of the Yolo algorithm, with a focus on the innovations and contributions made by each iteration, as well as the major changes in network architecture (and training tricks) which have been implemented over time.\n\n\n\n\nA timeline of YOLO development\n\n\n\n4.2.1 Applications of YOLO\nYolo has proven invaluable for a number of different applications\n\nautonomous vehicles\n\nenables quick identification and tracking of objects like vehicles, pedestrians, bicycles and other obstacles\n\naction recognition\nvideo surveillance\nsports analysis\nhuman-computer interaction\ncrop, disease, pest detection and classsification\nface detection - biometrics, security, facial recognition\ncancer detection\nskin segmentation\npill identification\nremote sensing\n\nsatellite and aerial imagery object detection / classification\nland use mapping\nurban planning\n\nsecurity systems\nsmart transportation systems\nrobotics and drones\n\n\n\n4.2.2 Evaluation Metrics\nAverage Precision (AP) and Mean Average Precision (mAP) are the most common metrics used in the object detection task.  It measures average precision across all categories, providing a single value to compare different models \n\n4.2.2.1 How AP works\n\nmAP is the average precision for accuracy of predictions across all classes of objects contained within an image\n\nindividual AP values are determined for each category separately.\n\nIOU (intersection over union)\n\nmeasures the proportion of the predicted bounding box which overlaps which overlaps with the true bounding box\n\n\n\n\n\nIntersection over union in practice\n\n\nDifferent methods are used to compute AP when evaluating object detection methods on the COCO and VOC datasets (PASCAL-VOC)\n\n\n\n4.2.3 Non-Maximum Suppression\nA post-processing technique - reduces number of overlapping boxes and improves detection quality. Object detectors typically generate multiple bounding boxes around the same object. Non-max suppression picks the best ones and gets rid of the others.\nThe algorithm for this is defined below:\n\n\n\nNon-Max Suppression Alg\n\n\nA useful visualization is also provided:\n\n\n\nNon-Max Supression Vis\n\n\n\n\n4.2.4 YOLO\nThe original authors of YOLO titled it as such for the reason that it only required a single pass on the image to accomplish the detection task. This is contrast to the other approaches used by Fast R-CNN and sliding window methods.\nThe output coordinates of the bounding box were detected using more straightforward regression techniques\n\n4.2.4.1 YOLOv1\n PASCAL-VOC AP: 63.4% \nYOLOv1 predicted all bounding boxes simultaneously by the following process:\n\ndivide image into S \\times S grid\npredict B bounding boxes of the same class and confidence for C different classes per grid element\neach bounding box had five values:\n\nPc - confidence score for the bounding box - how likely it contains an object and the accuracy of the box\nbx and by - coordinates of center of box relative to grid cell.\nbh and bw - height and width of box relative to full\n\noutput an S \\times S \\times (B \\times 5 + C) tensor\n(optional) NMS used to remove redundant bounding boxes\n\nHere is an example of that output:\n\n\n\nyolo output prediction\n\n\n\n4.2.4.1.1 v1 Architecture\nNormal Architecture\n\n 24 conv layers \n\n1 \\times 1 conv layers are used - reduce number of feature maps and keep parameters lower\n leaky rectified linear unit activations\n\n 2 fc layers \n\npredict bounding box coordinates / probs\n linear activation function for final layer \n\n\nFastYOLO\n\nUsed 9 conv layers instead of 24 for greater speed (at the cost of reduced accuracy)\n\n\n\n\nyolo v1 architecture\n\n\n\n\n4.2.4.1.2 v1 Training\nBasic training process:\n\npretrain first 20 layers at resolution 224 \\times 224 with ImageNet dataset\nadd last four layers with randomly initialized weights - fine tune model with PASCAL VOC 2007 and PASCAL VOC 2012 at resolution 448 \\times 448\n\nLoss functions:\n\nscaling factors\n\n\\lambda_{coord} = 5 - gives more weight to boxes with objects\n\\lambda_{noobj} = 0.5 - reduces importance of boxes with no object\n\nlocalization loss:\n\nfirst two terms\ncomputes error in predicted bounding box locations (x,y) and (w,h)\nonly penalizes boxes with objects in them\n\nconfidence loss:\n\nconfidence error when object is detected (third term)\nconfidence error when no object is in box (fourth term)\n\nclassification loss:\n\nsquared error of class conditional probabilities for each class if an object appears in the cell\n\n\n\n\n\nyolo v1 loss function\n\n\n\n\n\n4.2.4.2 YOLOv2 (YOLO 9000)\n PASCAL-VOC AP: 78.6% \nImprovements / Changes\n\nBatch normalization - included on all convolutional layers\nHigher resolution classifier - pretrained model (224 x 224) and then fine-tuned with ImageNet at a higher reoslution (448 x 448) for ten epochs\nfully convolutional - remove dense layers and use fully conv architecture\nuse anchor boxes to predict bounding boxes\n\nanchor box - box with predefined shapes for prototypical objects\ndefined for each grid cell\nsystem predicts coordinates and class for every anchor box\n\n\n\n\n\nyolo v2 anchor boxes\n\n\n\nDimension clusters - pick good anchor boxes using k-means clustering on the training bounding boxes - improves accuracy of bounding boxes\nDirect Location Prediction\nFiner-grained features\n\nremoved pooling layer - get feature 13 x 13 feature map for 416 x 416 images\npassthrough layer - 26 x 26 x 512 feature map -&gt; stack adjacent features into different channels\n\nMulti-scale training - train on different input sizes to make model robust to different input types\n\n\n4.2.4.2.1 v2 Architecture\n\nbackbone architecture -&gt;  Darknet-19 \n\n19 conv layers\n5 max pool layers\n\nnon-linear operation - uses OT to perform efficiently\n\nuse 1 \\times 1 conv between 3 \\times 3 to reduce parameters\nbatch normalization to help convergence\nobject classification head (replaces last 4 conv layers of YOLOv1)\n\n1 conv layer (1000 filters)\nGAP layer\nSoftmax classifier\n\n\n\n\n\n\nyolo v2 architecture\n\n\n\n\n\n4.2.4.3 YOLOv3\n MS COCO AP: 36.2% AP(50): 60.6% \nThe code used to run YOLOv3 in Torch is provided at this repository\n\n4.2.4.3.1 YOLOv3 Architecture\nYOLOv3 makes use of a larger network architecture (backbone) called Darknet-53.\n\nreplaces all max-pooling layers with strided convolutions and added residual connections (what are residual connections?) - see Chapter 3 for more information on this primitive.\n\n\n\n\nyolo v3 architecture\n\n\nThe darknet architecture is presented here as well (visually):\n\n\n\ndarknet 53 backbone\n\n\n\n\n4.2.4.3.2 Multi-Scale predictions\n\nenables multi-scale predictions (predictions at multiple grid sizes)\nthis helps to obtain finer detailed boxes (improves prediction of smaller boxes)\nYOLOv3 generates three separate outputs:\n\ny1: 13 \\times 13 grid defines the output\ny2: concatenating output after (Res \\times 4) with output of (Res \\times 8) - upsampling occurs from y1 since the feature maps are of different sizes (13 \\times 13) and (26 \\times 26)\ny3: upsample y2 output to match 52\\times 52 feature maps\n\n\n\n\n4.2.4.3.3 Backbone, Neck, and Head\nAfter release of YOLOv3, object detectors began to be described in terms of the backbone, neck, and head\n\n\n\nmodern object detection architecture\n\n\nBackbone\n\nExtracts useful features from the input image.\nA convolutional nerual network trained on large-scale image classifications task (ImageNet)\ncaptures hierarchical features at different scales\n\nlow-level features - earlier layers\nhigh-level features - deeper layers\n\n\nNeck\n\naggregates / refines features extracted by backbone\n\nenhance spatial / semantic information across different scales\nincludes conv layers\nincludes feature pyramid networks\n\n\nHead\n\nmakes predictions based on features provided by backbone and neck\nconsists of task-specific subnetworks to perform classification, localization, localization, instance segmentation and pose estimation\nnon-maximum supression filters out overlapping predictions (retains only most confident detections)\n\n\n\n\n4.2.4.4 YOLOv4\n MS COCO AP: 43.5% AP(50): 65.7% \nThe philosophy of YOLOv4 approaches optimization of the model into two categories: bag of freebies and bag of specials\nBag of Freebies:\n\nincrease training time / cost\ndo not affect inference time\nexamples include data augmentation\n\nBag of Specials:\n\nincrease inference time / cost\nimprove accuracy of the model (MaP)\nexamples include\n\n enlarging receptive field ??? \n combining features ??? \n post-processing ??? \n\n\n\n4.2.4.4.1 Model Improvements\n\nEnhanced network architecture with Bag-of-Specials\n\nbackbone: Darknet-53 +  Cross-stage Partial Connections (CSPNet) ??  +  Mish Activation Function ?? \n\ntested several backbone architectures to choose best option\nCSP reduces computation while maintaining accuracy\n\nneck:  Spatial Pyramid Pooling (SPP)??  + Multi-scale predictions + modified  path aggregation network PANet  +  modified spatial attention module (SAM) \n\nSPP increases receptive field without affecting inference speed\n\ndetection head: Same anchors as YOLOv3\n\nAdvanced training with Bag-of-Freebies\n\nregular augmentation\n\nrandom brightness, contrast, scaling, cropping, flipping, rotation\n\nspecial\n\nmosaic augmentation:\n\ncombines four images into a single one\nreduces need for large mini-batches for batch normalization\n\nDropBlock regularization (instead of Dropout)\nCIoU loss and Cross mini-batch nomralization CmBN for collecting statistics from the entire batch instead of just mini-batches\n\nthese changes improve the detector\n\n\n\nSelf-adversarial Training\n\nimproves model robustness to perturbations\n\nHyperparameter optimization with Genetic Algorithms\n\n genetic algs  used on first 10% of periods\n cosine annealing scheduler  to alter learning rate during training\n\n\n\n\n4.2.4.4.2 YOLOv4 Architecture\n\n\n\nyolo v4 architecture\n\n\n\n\n\n4.2.4.5 YOLOv5\n YOLOv5x MS COCO AP: 50.7% AP(50): X% \nNo paper exists for YOLOv5 but there are several key advantages to using it\n\nIt is developed in PyTorch instead of Darknet\nis open source and actively maintained by ultralytics\nis easy to use, train, and deploy\nmany integrations for labeling, training, and deployment (mobile)\n\nThere are several scaled versions of this model:\n\nYOLOv5n (nano)\nYOLOv5s (small)\nYOLOv5m (medium)\nYOLOv5l (large)\nYOLOv5x (extra large)\n\nThis is useful for V2X privacy preserving applications because we will want to make use of smaller networks than might otherwise be used and compare their performance (efficiency) against larger networks.\nThis helps to answer the question of whether the accuracy tradeoff is worth the speedup.\n\n4.2.4.5.1 Results\nYOLOv5x achieves the following results\n\nAP 50.7% on COCO [batch size = 32, 640 pixels]\nAP 55.8% on COCO [1536 pixels]\n\n\n\n\n4.2.4.6 Scaled-YOLOv4\n YOLOv4-large MS COCO AP: 55.6% \nUtilized scaling techniques (making larger to increase accuracy at the expense of speed, and scaling down increases speed at the expense of accuracy).  Scaled-down models require less compute power and can run on embedded systems \nLike YOLOv5, was developed in PyTorch instead of Darknet\n\n4.2.4.6.1 Results\n\nYOLOv4-large - MS COCO AP: 56%\nYOLOv4-tiny - MS COCO AP: 22%\n\n\n\n\n4.2.4.7 YOLOR\n MS COCO AP: 55.4% AP(50): 73.3% \nYOLOR stands for “you only learn one representation” and is novel for introducing multi-task learnin approach which creates a single model for multiple tasks (classification, detection, pose estimation) by learning a general representation and using sub-networks to create task-specific representations!\n\n4.2.4.7.1 Results\n\nYOLOR - MS COCO AP: 55.4% AP(50): 73.3%\n\n\n\n\n4.2.4.8 YOLOX\n MS COCO AP: 50.1% \nWas designed off the back of the Ultralytics YOLOv3 (see Section 4.2.4.3) in PyTorch.\n\n4.2.4.8.1 Model Changes and Improvements\n\nAnchor Free - simplified training and decoding process\nMulti Positives - center sampling (assigned center 3 \\times 3 area as positives) to account for imbalances produced by lack of anchors\nDecoupled Head - classification confidence and localization accuracy separated into two heads (connecting them leads to some misalignment)\n\nsped up model convergence and improved accuracy\n\nAdvanced Label Assignment - ambiguities were associated with ground truth bounding boxes (box overlap) - this is addressed with assigning procedure as  Optimal Transport Problem . The authors develop a simplified version called  simOTA \nStrong augmentations -  MixUP  and  Mosaic augmentations  were used\n\n\n\n4.2.4.8.2 Architecture\n\n\n\nyolox architecture\n\n\n\n\n\n4.2.4.9 YOLOv6\n MS COCO AP: 52.5% AP(50): 70% \nAdopted an anchor-free detector and provided a series of models at different scales for nuanced applications.\n\n4.2.4.9.1 Model Changes and Improvements\n\nNew architecture backbone based on  RepVGG .  higher parallelism  and use of neck based on  RepBlocks  or  CSPStackRep Blocks  and developed an efficient decoupld head\nLabel Assignment with  TOOD \nNew classification and regression losses using  VariFocal loss  and SIoU / GioU regression loss \nself-distillation for regression and classification tasks\nquantization scheme for detection with  RepOptimizer   channel-wise distillation  for a faster detector\n\n\n\n\n4.2.4.10 YOLOv7\n MS COCO AP: 55.9% AP(50): 73.5% - input 1280 pixels \nCompared to YOLOv4 and YOLOR, YOLOv7 achieves a significant reduction in parameters used, and improves average precision by a meaningful increase in average precision as well.\n\n4.2.4.10.1 Model Changes and Improvements\nArchitectural Changes\n\nExtended efficient layer aggregation network -  ELAN  allows for more efficient training and convergence\nModel Scaling for concatenation-based models - maintain optimal structure of the model by scaling thedepth and width of the block with the same factor\n\nBag-of-Freebies\n\nPlanned re-parameterized convolution - Architecture is inspired by  RepConv . Identity connection outperforms residual in Resnet (Chapter 3) and  concatenation in DenseNet \nCoarse label assignment for auxiliary head (training) and fine label assignment for the lead head (final out)\nBatch normalization in conv-bn-activation - integrates mean and variance of batch normalization into weight and bias of convolutional layer at inference stage (batch norm folding)\nImplicit knowledge (see Section 4.2.4.10)\nExponential moving average as final inference model\n\n\n\n\n4.2.4.11 DAMO-YOLO\n MS COCO AP: 50.0% \nWas inspired by a series of technologies relevant at the time and provided tiny/small/medium scaled model variants\n\nNeural architecture search (this was also used in Delphi) -  MAE NAS  -\na large neck\na small head\n aligned OTA label assignment  - uses focal loss for classification cost and IoU of prediction / ground truth box as soft label. Enables selection of aligned samples for targets\n knowledge distillation \n\n\nteacher guiding student (stage 1)\n\n\nstudent fine-tuning (stage 2)\n\nenhancements in distillation approach\n\nalign module - adapts student features to same resolution as teacher’s\nchannel-wise dynamic temperature - normalizes teacher and student features\n\n\n\n\n\n4.2.4.12 YOLOv8\n YOLOv8x MS COCO AP: 53.9% (640 pixels) \nA version of YOLO released by ultralytics which is anchor free and uses mosaic augmentation for training up to the last ten epochs. It provides five scaled versions\n\nYOLOv8n (nano)\nYOLOv8s (small)\nYOLOv8m (medium)\nYOLOv8l (large)\nYOLOv8x (extra large)\n\n\n\n\n4.2.5 PP-YOLO Models\nThe PP-YOLO models were developed in parallel with the standard YOLO variants. These models were based on YOLOv3 but were developed in the  PaddlePaddle  deep learning platform instead. The goal of their work was to show how an object detector should be constructed step-by-step, not to provide any novel functionality or a new approach.\n\n4.2.5.1 PP-YOLO\n MS COCO AP:45.9% AP(50): 65.2% \n\n4.2.5.1.1 Divergence from YOLOv3\n\n ResNet50-vd backbone  + deformable convolutions replace DarkNet-53 (Section 4.2.4.3) - achieves higher classification accuracy on Imagenet\nLarger batch size - improves training stability\nMaintained moving averages for trained parameters\n DropBlock  applied on FPN\nIoU loss added with L1-Loss for bounding box regression\nIoU prediction branch for measuring localization accuracy (and optimization)\nGrid Sensitive Approach like YOLOv4 (Section 4.2.4.4) - improves bounding box center prediction at grid boundary\nMatrix NMS (parallelized NMS for faster computation)\nCoordConv - 1\\times 1 convolution of the FPN. Allows for learning  translational invariance \nSpatial Pyramid Pooling increases receptive field of the backbone\n\n\n\n4.2.5.1.2 PP-YOLO Augmentations and Preprocessing\n\nMixup Training - weights \\sim Beta(\\alpha=1.5, \\beta=1.5)\nRandom color distortion\nRandom expand\nRandom crop and random flip with probability 0.5\nRGB channel z-score normalization \\mu = [0.485, 0.456, 0.406] and \\sigma=[0.229,0.224,0.225]\n\n\n\n\n4.2.5.2 PP-YOLOv2\n MS COCO AP: 49.5% \n\n4.2.5.2.1 Changes and Improvements\n\nBackbone changed from ResNet50 to ResNet101\nPath aggregation network instead of FPN\nMish Activation function applied to the detection neck\nLarger input sizes - increases performance on small objects\nModified IoU aware branch - soft label format instead of soft weight format\n\n\n\n\n4.2.5.3 PP-YOLOE\n MS COCO AP: 51.4% (78.1 FPS NVIDIA V100) \n\n4.2.5.3.1 Changes and Improvements\n\nAnchor Free (following trends of other yolo models)\nNew backbone and neck - modified neck wtih  RepResBlocks (combine dense and residual connections)\ntask alignment learning (see Section 4.2.4.8)\nefficient task-aligned head (ET-head) - single head based on  TOOD  instead of splitting the classification / detection heads like with YOLOX (Section 4.2.4.8)\n varifocal (VFL)  and distribution focal loss (DFL)\n\nVFL weights positive samples using a target score which places a greater importance on high-quality samples during training\nDFL extends Focal Loss from discrete to continuous labels - better for representations which combine qualtiy estimation and class prediction - this allows for better depiction of flexible distributions in real data (eliminates risk of inconsistency)\n\n\nThe authors provide several scaled modesl\n\nPP-YOLOE-s (small)\nPP-YOLOE-m (medium)\nPP-YOLOE-1 (large)\nPP-YOLOE-x (extra large)\n\n\n\n\n\n4.2.6 Summary of YOLO\n\n\n\nsummary of yolo architectures and results\n\n\n\n\n\n\nJ. Terven, D. Cordova-Esparaza. 2023. “A Comprehensive Review of YOLO: From YOLOv1 to YOLOv8 and Beyond.” ACM Computing Surveys. https://arxiv.org/pdf/2304.00501v1.pdf.\n\n\nW. Liu, D. Erhan, D. Anguelov. 2016. “SSD: Single Shot MultiBox Detector.” ECCV."
  },
  {
    "objectID": "v2x_apps.html",
    "href": "v2x_apps.html",
    "title": "V2X Applications",
    "section": "",
    "text": "This portion of the text deals specifically with discussions of V2X applications and papers which have tackled these sorts of the problems in the past.",
    "crumbs": [
      "V2X Applications"
    ]
  },
  {
    "objectID": "red_light.html#motivation",
    "href": "red_light.html#motivation",
    "title": "5  Red Light Violation Detection",
    "section": "5.1 Motivation",
    "text": "5.1 Motivation\nSecure Red Light Violation detection is an important application of secure machine learning protocols. Oftentimes, these systems will require the use of image segmentation and object recognition protocols. The images used for this task expose often-times sensitive data about individual users.\nIn the practical setting of interest, this means exposing license-plate information, associated vehicles, and location information available from the images themselves."
  },
  {
    "objectID": "red_light.html#v2i-algorithms-for-rlr-detection",
    "href": "red_light.html#v2i-algorithms-for-rlr-detection",
    "title": "5  Red Light Violation Detection",
    "section": "5.2 V2I Algorithms for RLR Detection",
    "text": "5.2 V2I Algorithms for RLR Detection\n\nThe authors of this paper have constructed V2I mechanisms for red light running (RLR) detection, wrong way entry (WWE), and an array of other import tasks in the context of V2X. See the citation Dokur and Katkoori (2022)\n\n\n5.2.1 Red Light Violation Detection Algorithm\nThe proposed system utilizes the following logic to detect whether a car will violate a red light.  A car which is approaching an intersection is connected to road-side units (RSUs)  which are installed at traffic lights in an intersection.\nEach light is said to be located at points B(x_{2}, y_{2}, z_{2}), C(x_{3}, y_{3}, z_{3}), D(x_{4}, y_{4}, z_{4}) and E(x_{5}, y_{5}, z_{5}) respectively.\nUnlike image-based systems, this system  assumes V2I communication between the traffic lights and the vehicle in question.  This means that the traffic state does not need to be determined by an image classifier. Rather, we already have this information by default."
  },
  {
    "objectID": "red_light.html#thao-et.al-on-traffic-violation-detection",
    "href": "red_light.html#thao-et.al-on-traffic-violation-detection",
    "title": "5  Red Light Violation Detection",
    "section": "5.3 Thao et.al on Traffic Violation Detection",
    "text": "5.3 Thao et.al on Traffic Violation Detection\n\nThe paper proposed by L. Thao (2022) introduces a mechanism for detecting red light violations automatically. There paper is titled: Automatic Traffic Red-Light Violation Detection Using AI\n\n\n5.3.1 Problem Setting\nThe reason AI technologies (image classification and detection) systems are better suited than standard sensor technologies is that they are able to operate more consistently, even when the number of vehicles in the setting increases dramatically.\n\n\n5.3.2 System Design and Solution Approach\nSeparate the task into three parts:\n\nvehicle violation detection\nred signal change monitoring\nvehicle recognition\n\nVehicle Violation Detection\nThe  YOLOv5s pretrained model (COCO dataset)  is used for detecting violating vehicles. After detecting violation, following frames are used to try and determine the license plate (identfy vehicle). See Section 4.2.4.1 for more information on the YOLO object detection model.\nBelow, a picture of the overall system flow is presented:\n\n\n\nsystem flow\n\n\n\nvehicle tracking - performed every 5 frames\n\nif IOU (intersection over union) of bounding box is close to one from a previous frame, then the car is assumed to be the same one from that frame.\n\nviolation line detection\n\nimage processing is used to determine traffic lines\nboundary lines are drawn onto frames captured by the camera later\n\ntraffic state detection\n\ncolor filters and image processing used to detect changes in the state of the traffic light\n\n\n\n\n5.3.3 Primary Contributions\n\nImplementation of modified YOLOv5s model\n\nused parameter changes from original model\nachieved following accuracy results:\n\n 82% - vehicle identification \n 90% - traffic signal status change \n 86% - violation detection \n\n\nBest Performing Architecture given below (v3 / v4)\n\n\n\n\nmodified Yolo architecture\n\n\nAccording to a labmate, it may only be necessary to fine tune the YOLO model on 10 epochs. It may not be necessary to do any more than that."
  },
  {
    "objectID": "red_light.html#goma-et.al-on-rlr-detection-with-ssd-single-shot-detector",
    "href": "red_light.html#goma-et.al-on-rlr-detection-with-ssd-single-shot-detector",
    "title": "5  Red Light Violation Detection",
    "section": "5.4 Goma et.al on RLR Detection with SSD (Single Shot Detector)",
    "text": "5.4 Goma et.al on RLR Detection with SSD (Single Shot Detector)\n\nWork by J. Goma (2020) demonstrates the ability to detect red-light-running and over-speeding with a high level of accuracy. Specifically, they achieve 100% accuracy on red light running detection and 92.1% accuracy for over-speeding violations. They accomplish these results using a CNN applied to an SSD (single deep neural network)\n\n\n5.4.1 Methods and System Design\n\n\n\n\nDokur, O., and S. Katkoori. 2022. “Vehicle-to-Infrastructure Based Algorithms for Traffic Light Detection, Red Light Violation, and Wrong-Way Entry Applications.” IEEE International Symposium on Smart Electronic Systems.\n\n\nJ. Goma, R. Bautista, M. Eviota. 2020. “Detecting Red-Light Runners (RLR) and Speeding Violtion Through Video Capture.” IEEE 7th International Conference on Industrial Engineering and Applications.\n\n\nL. Thao, N. Anh, D. Cuong. 2022. “Automatic Traffic Red-Light Violation Detection Using AI.” International Information and Engineering Technology Association."
  },
  {
    "objectID": "traffic_flow.html#attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecasting",
    "href": "traffic_flow.html#attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecasting",
    "title": "6  Traffic Flow Forecasting",
    "section": "",
    "text": "In this paper, S. Guo (2019) proposed a method for traffic flow prediction which utilized an attention based spatial-temporal graph convolutional network. They aimed to model several time-dependencies: (1) recent, (2) daily-periodic, and (3) weekly-periodic dependencies. The attention mechanism captures spatial-temporal patterns in the traffic data and the spatial-temporal convolution is used to capture spatial patterns while standard convolutions describe temporal features.\n\n\n6.1.1 Core Contributions of ASTGCN\nDifficulties of the traffic forecasting problem\n\nit is difficult to handle unstable and nonlinear data\nprediction performance of models require extensive feature engineering\n\ndomain expertise is necessary\n\ncnn - spatial feature extraction from grid-based data, gcn - describe spatial correlation of grid based data\n\nfails to simultaneously model spatial temporal features and dynamic correlations of traffic data\n\n\nAddressing these issues:\n\ndevelop a spatial-temporal attention mechanism\n\nlearns dynamic spatial-temporal correlations of traffic data\ntemporal attention is applied to capture dynamic temporal correlations for different times\n\nDesign of spatial-temporal convolution module\n\nhas graph convolution for modeling graph structure\nhas convolution in temporal dimension (kind of like 3-d convolution)\n\n\n\n\n\n\nS. Guo, N. Feng, Y. Lin. 2019. “Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting.” The Thirty-Third AAAI Conference on Artificial Intelligence.",
    "crumbs": [
      "V2X Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Traffic Flow Forecasting</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Beaver, D. 1991. “Efficient Multiparty Protocols Using Circuit\nRandomization.” Advances in Cryptology. https://link.springer.com/chapter/10.1007/3-540-46766-1_34.\n\n\nDokur, O., and S. Katkoori. 2022. “Vehicle-to-Infrastructure Based\nAlgorithms for Traffic Light Detection, Red Light Violation, and\nWrong-Way Entry Applications.” IEEE International Symposium\non Smart Electronic Systems.\n\n\nJ. Goma, R. Bautista, M. Eviota. 2020. “Detecting Red-Light\nRunners (RLR) and Speeding Violtion Through Video Capture.”\nIEEE 7th International Conference on Industrial Engineering and\nApplications.\n\n\nJ. Terven, D. Cordova-Esparaza. 2023. “A Comprehensive Review of\nYOLO: From YOLOv1 to YOLOv8 and Beyond.” ACM Computing\nSurveys. https://arxiv.org/pdf/2304.00501v1.pdf.\n\n\nL. Thao, N. Anh, D. Cuong. 2022. “Automatic Traffic Red-Light\nViolation Detection Using AI.” International Information and\nEngineering Technology Association.\n\n\nS. Guo, N. Feng, Y. Lin. 2019. “Attention Based Spatial-Temporal\nGraph Convolutional Networks for Traffic Flow Forecasting.”\nThe Thirty-Third AAAI Conference on Artificial Intelligence.\n\n\nW. Liu, D. Erhan, D. Anguelov. 2016. “SSD: Single Shot MultiBox\nDetector.” ECCV.\n\n\nWong, W. 2021. “What Is Residual Connection?” https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55."
  },
  {
    "objectID": "traffic_flow.html",
    "href": "traffic_flow.html",
    "title": "6  Traffic Flow Forecasting",
    "section": "",
    "text": "6.1 Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting",
    "crumbs": [
      "V2X Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Traffic Flow Forecasting</span>"
    ]
  },
  {
    "objectID": "secure_prim/beavers_triples.html",
    "href": "secure_prim/beavers_triples.html",
    "title": "1  Beaver’s Triples Explained",
    "section": "",
    "text": "1.1 Generating Boolean Beaver’s Triples\nThe first step in masking the AND operation is to generate Boolean Beaver’s Triples. These are random bits a, b, and c such that c = a \\cdot b, where \\cdot represents the AND operation. Throughout this tutorial we define a = a_A \\oplus a_B, b = b_A \\oplus b_B, c = c_A \\oplus c_B, and r = r_A \\oplus r_B.",
    "crumbs": [
      "Secure Multiparty Computation Primitives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Beaver's Triples Explained</span>"
    ]
  },
  {
    "objectID": "secure_prim/beavers_triples.html#generating-boolean-beavers-triples",
    "href": "secure_prim/beavers_triples.html#generating-boolean-beavers-triples",
    "title": "1  Beaver’s Triples Explained",
    "section": "",
    "text": "1.1.1 Step 1: Generating Inputs\nSuppose we have two parties, Alice and Bob. We begin by first generating a random pair (a_{i}, b_{i}) for each party where i corresponds to the party in question. We generate these from the field F_{2}^{2} which simply means that we select two random bits from \\{0,1\\}.\n\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\mathbb{F}_{2}^{2} \\overset{\\$}{\\rightarrow} (a_{A}, b_{A}) & \\space & \\mathbb{F}_{2}^{2} \\overset{\\$}{\\rightarrow} (a_{B}, b_{B})\n\\end{array}\n\nNext, Bob and Alice each generate random masking bits r_{A} and r_{B} respectively.\n\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\mathbb{F}_{2} \\overset{\\$}{\\rightarrow} r_{A} & \\space & \\mathbb{F}_{2} \\overset{\\$}{\\rightarrow} r_{B}\n\\end{array}\n\n\n\n1.1.2 Step 2: Obtaining Shares of c\nAfter generating the masking bits, Alice and Bob invoke an instance of 2 \\choose 1-OT_2 (see Chapter 2), where Alice acts as the sender with inputs (r_{A}, r_{A} \\oplus a_{A}), and Bob acts as the receiver with input b_B. Bob obtains b_{B}a_{A} \\oplus r_{A}.\nNow, Alice and Bob invoke another instance of 2 \\choose 1-OT_2. This time, Bob serves as the sender with inputs (r_{B}, r_{B} \\oplus a_{B}), and Alice serves as the receiver with input b_A. Consequently, Alice obtains b_{A}a_{B} \\oplus r_{B}.\n\n\\begin{array}{ccc}\n    \\text{Alice} & {} & \\text{Bob} \\\\\n    \\text{Snd } = (r_{A}, r_{A}\\oplus a_{A}) & \\overset{{2 \\choose 1}\\text{-OT}_2}{\\longrightarrow} & \\text{Rcv } = b_{B}a_{A} \\oplus r_{A} \\\\\n    \\text{Rcv } = b_{A}a_{B} \\oplus r_{B} & \\overset{{2 \\choose 1}\\text{-OT}_2}{\\longleftarrow} & \\text{Snd } = (r_{B}, r_{B} \\oplus a_{B})\n\\end{array}\n\nNote that objective is for Alice and Bob to each hold shares of c, denoted as c_A and c_B, respectively, such that their combination satisfies the equation c = ab.\nAlice had received b_{A}a_{B} \\oplus r_{B} from Bob, and she already has b_{A}a_{A} \\oplus r_{A}. She computes b_{A}a_{B} \\oplus r_{B} \\oplus b_{A}a_{A} \\oplus r_{A}, which simplifies to b_{A}a \\oplus r. Alice sets c_A = b_{A}a \\oplus r.\nSimilarly, Bob computes b_{B}a_{A} \\oplus r_{A} \\oplus b_{B}a_{B} \\oplus r_{B}, which simplifies to b_{B}a \\oplus r. Bob sets c_B = b_{B}a \\oplus r.\n\n\\begin{array}{ccc}\n    \\text{Alice} & & \\text{Bob} \\\\\n    c_A = b_{A}a \\oplus r & \\space & c_B = b_{B}a \\oplus r\n\\end{array}\n\nIt is easy to see that c = c_A \\oplus c_B = b_{A}a \\oplus r \\oplus b_{B}a \\oplus r = ab.\nThus, we have shown how Boolean Beaver’s triples are randomly generated such that c = c_A \\oplus c_B.\n\n\n\n\nBeaver, D. 1991. “Efficient Multiparty Protocols Using Circuit Randomization.” Advances in Cryptology. https://link.springer.com/chapter/10.1007/3-540-46766-1_34.",
    "crumbs": [
      "Secure Multiparty Computation Primitives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Beaver's Triples Explained</span>"
    ]
  }
]